---- containers
- each container should run only one process
- ENTRYPOINT[], executed when container is started, run your app
- CMD[], arguments to pass to ENTRYPOINT
- filesystem is isolated per container
- runAsUser: root by default
- container user overwrites pod's user
- --cap-add/drop to add or remove host/linux capabilities

docker build -t semirm/mysvc:1.0.0 . # -f cmd/mysvc/Dockerfile
docker run --name mysvc -d -p 8080:8080 semirm/mysvc:1.0.0
docker login
docker push semirm/mysvc:2

docker-compose up --force-recreate --build
docker exec -it 97e37cc2d825 sh

-- minikube
brew install hyperkit
minikube start --vm=true --driver=hyperkit # required for MacOS
minikube addons enable ingress

kubectl get ingress # get address
# add to /etc/hosts: 172.17.0.15 mysvc.com

----

oci: open container initiative (define standards)
cri: container runtime interface
runc: the container runtime, low-level cri implementation (kata, gvisor...), runs/builds containers
--
client (docker-cli/docker-compose/podman-cli): for end users, talks to deamons via api
|
deamon/engine (containerd-cri/cri-o/dockerd): pull images, networking, storage, api, talks to runtime
|
runtime (runc/kata/gvisor): low level, create and run containers

---------------------------------------------------

------ pods
- group multiple containers together as a single unit
- multiple containers in same pod share same linux network and uts namespace but not filesystem
- all pods in all nodes can see each other, same network space, no NAT needed
- pods are single scalable units, can not scale single container in pod, only a whole pod
- use volumes to share data across all containers in a pod
- volume is defined in pod definition
- volumeMount is used in container definition to mount volume
- mounting directories in volumeMount will completely override existing directory, use subPath in volumeMount
- volume lifecycle is tied to pod (most of volume types), if pod dies then volumes die also
- nodeSelector for simple pod:node matching
- affinity:matchExpressions for complex pod:node matching

- volume types:
-- emptyDir{}, empty directory, tied to pod lifecycle
-- gitRepo{}, emptyDir{} with github repository branch as initial content, tied to pod lifecycle
-- hostPath{}, filesystem from node (host) where pod is scheduled, tied to node lifecycle
-- NAS (network attached storage) volumes | NFS (network file system):
--- types: gcePersistentDisk, awsElasticBlockStore, azureFile, azureDisk...
--- lifecycle not tied to node/pod, storage on remote location (cloud)
--- best option to store persistent data, it will survive node/pod crash
-- PersistentVolume{}, nfs volume definitions, cluster level, not used in pods directly
-- PersistentVolumeClaim{}, used in pod definitions to request PersistentVolume
-- configMap{}, each entry of ConfigMap will create a file to container, all files are updated automatically (but not in processes)
-- secret{}, stored on tmpfs (memory), encrypted, stored only on nodes that run pods that need access to

- service account
two types of accounts: user (admin, developer) and service (k8s api, prometheus, jenkins)
each namespace has default service account with limited access
service account requires secret token (<1.22 token automatically created, >1.24 requires manual creation)
each pod is by default assigned volume from default service account

- node taint
kubectl taint nodes node1 [params] to organize pods scheduling
taint tells node to accept specific pods, but it doesnt mean pod will be scheduled on tainted node! 
add toleration config to pod definition to tell pod to tolerate specific node taint

- taint/toleration vs affinity
-- taint/toleration, node side, protect node from pods 
-- affinity, pod side, protect pod being placed on node

- patterns
-- sidecar, logs, send logs to log server
-- adapter, middleware between sidecar container and destination (log server)
-- ambassador, proxy container between pod and external destination (db, access external svc as localhost)

- pod ready state control, when the application is ready to be used
- readiness probe, healthchecks, delays, interval to run, initially on pod start
- liveness probe, continues healthchecks after pod is deployed, after container crashes/restarts

---------------------------------------------------

use initContainer for requirements setup (db create...)
k describe pod -> conditions to check pod scheduling conditions

---- filter resources
$ kubectl get pods -o json | jq '.items[0].status.initContainerStatuses[] | {image: .image, imageId: .imageID}'
$ kubectl get pods -o json | jq '[.items[].status.initContainerStatuses[]? | {image: .image, imageId: .imageID}]'
$ kubectl get pods -o json | jq '[.items[].status.initContainerStatuses[]? | select(.image == "mydocker/image") | {image: .image, imageId: .imageID}]'


kubectl exec -it mysvc-2356236189-frggk -- bash

---- rollout deployment
kubectl rollout restart deployment <deployment_name>, force deployment
kubectl rollout undo deployment <deployment_name>, revert last deployment
kubectl rollout status deployment <deployment_name>
kubectl rollout history deployment <deployment_name>
kubectl rollout undo deployment <deployment_name> --to-revision=1, revert to first deployment
kubectl rollout pause deployment <deployment_name>, canary-like deployment (not best practice, use new deployment instead)
kubectl rollout resume deployment <deployment_name>

- use maxSurge and maxUnavailable to control rate of rollout
- use minReadySeconds and readinesProbe to automatically block fault rollouts

kubectl config view, display config info, available clusters and contexts
kubectl config current-context <context_name>, set current context to use
kubectl config set-context --current --namespace=<mynamespace>, modify current-context, set default namespace

replicaSet, infinitely run pods
jobs, run pod until completed state, or completions cound matched
cronJobs, periodically run pods

- LoadBalancer service requires its own load balancer and publich IP
-- means n services = n load balancers + public IPs
- Ingress requires only one, host and path is used to forward traffic to different services
-- means n services = 1 Ingress (load balancer + IP)
-- multiple hosts (foo.example.com and bar.example.com) should/could point to same Ingress Controller IP - this is the difference from LoadBalancer
-- configure ingress controller (nginx, haproxy) + define ingress resource (k8s manifest)
-- ingress controller = 
--- k8s nginx-ingress-controller container deployment
--- service (nodeport), expose ingress controller to external world
--- service account, with some custom roles/permissions, requires to let nginx-controller do some intelligent logic
-- ingress resource =
--- define routing rules 


---------------------------------------------------

---- k8s operators
control loop
- software that tracks state of resource
- reconciliation loop: want x state -> check x state on cluster -> sync state

crd
- custom resource definition
- custom k8s object
- state tracked by controller

controller
- reconciliation logic
- tracks crd state
- updates resource state

operator
- crd + controller


---- create new operator
$ kubebuilder init  --repo opey -> init basic project structure
$ kubebuilder create api --group my-opey --version v1 --kind Opey -> create crd (go) and controller (go)
$ controller-gen <args> -> create k8s yaml manifest from go definitions
$ go run main.go -> run operator, or run as docker container
$ kubectl apply -f <config/crd/*.yaml> -> apply crd on k8s, verify with kubectl get crd


---- helm charts
- controller-gen <args>, generate crd.yaml from _types.go
- check values.yaml, will be used in template.yaml
- helm dependency update, get dep charts from charts.yaml to charts/
- helm package . -d build/, create helm chart from charts.yaml and charts/*
- helm upgrade --install <rel_name> <chart_chart>, apply charts
- helm rollback <rel_name> <rev_num>, revert release to specified revision