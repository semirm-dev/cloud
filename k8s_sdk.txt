-- k8s operators
control loop
- software that tracks state of resource
- reconciliation loop: want x state -> check x state on cluster -> sync state

crd
- custom resource definition
- custom k8s object
- state tracked by controller

controller
- reconciliation logic
- tracks crd state
- updates resource state

operator
- crd + controller


-- create new operator
$ kubebuilder init  --repo opey -> init basic project structure
$ kubebuilder create api --group my-opey --version v1 --kind Opey -> create crd (go) and controller (go)
$ controller-gen <args> -> create k8s yaml manifest from go definitions
$ go run main.go -> run operator, or run as docker container
$ kubectl apply -f <config/crd/*.yaml> -> apply crd on k8s, verify with kubectl get crd


-- helm charts
1.) Generate crds
$ controller-gen <args> -> generate config/crd/proj.yml manifest based on api/v1/proj_types.go

2.) Resolve values and create basic “template/chart” for each project
- copy values.tpl.yml to proj/values.yml
- copy chart.tpl.yml to proj/chart.yml
- replace tpl values

3.) Manifests and charts per project
$ helm dependency update -> create chart dependencies for project in charts/*.tgz, based on current charts.yml
$ helm template <args> -> create build/proj.yml manifest, k8s native manifest

4.) Build final helm chart, for entire solution (root project)
$ helm dependency update -> create dependencies for bundle chart in charts/*.tgz, based on current charts.yml
$ helm package . -d $buildDir -> create final helm bundled chart to be deployed to k8s, based on charts.yml and /charts/*

5.) Apply helm charts
$ helm upgrade --install <release_name> <chart_name/repo> (or helm install)

TLDR; helm charts
- controller-gen <args>, generate crd.yaml from _types.go
- check values.yaml, will be used in template.yaml
- helm dependency update, get dep charts from charts.yaml to charts/
- helm package . -d build/, create helm chart from charts.yaml and charts/*
- helm upgrade --install <rel_name> <chart_chart>, apply charts


-- filter resources
$ kubectl get pods -o json | jq '.items[0].status.initContainerStatuses[] | {image: .image, imageId: .imageID}'
$ kubectl get pods -o json | jq '[.items[].status.initContainerStatuses[]? | {image: .image, imageId: .imageID}]'
$ kubectl get pods -o json | jq '[.items[].status.initContainerStatuses[]? | select(.image == "mydocker/image") | {image: .image, imageId: .imageID}]'


kubectl config set-context --current --namespace=my-ns
kubectl exec -it mysvc-2356236189-frggk -- bash
kubectl rollout restart deployment <deployment name>
kubectl rollout status deployment <deployment name>


-- containers
- each container should run only one process
- ENTRYPOINT[], executed when container is started, run your app
- CMD[], arguments to pass to ENTRYPOINT
- filesystem is isolated per container

-- pods
- group multiple containers together as a single unit
- multiple containers in same pod share same linux network and uts namespace but not filesystem
- all pods in all nodes can see each other, same network space, no NAT needed
- pods are single scalable units, can not scale single container in pod, but the whole pod
- use volumes to share data across all containers in a pod
- volume is defined in pod definition
- volumeMount is used in container definition to mount volume
- mounting directories in volumeMount will completely override existing directory, use subPath in volumeMount
- volume lifecycle is tied to pod (most of volume types), if pod dies then volumes die also
- volume types:
-- emptyDir{}, empty directory, tied to pod lifecycle
-- gitRepo{}, emptyDir{} with github repository branch as initial content, tied to pod lifecycle
-- hostPath{}, filesystem from node (host) where pod is scheduled, tied to node lifecycle
-- NAS (network attached storage) volumes | NFS (network file system):
--- types: gcePersistentDisk, awsElasticBlockStore, azureFile, azureDisk...
--- lifecycle not tied to node/pod, storage on remote location (cloud)
--- best option to store persistent data, it will survive node/pod crash
-- PersistentVolume{}, nfs volume definitions, cluster level, not used in pods directly
-- PersistnVolumeClaim{}, used in pod definitions to request PersistentVolume
-- configMap{}, each entry of ConfigMap will create a file to container, all files are updated automatically (but not in processes)
-- secret{}, stored on tmpfs (memory), encrypted, stored only on nodes that run pods that need access to
